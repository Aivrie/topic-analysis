{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import ldamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_week51 = pd.read_csv(r'corpus_week51.csv', delimiter=',')\n",
    "texts_week52 = pd.read_csv(r'corpus_week52.csv', delimiter=',')\n",
    "texts_week53 = pd.read_csv(r'corpus_week53.csv', delimiter=',')\n",
    "texts_week1 = pd.read_csv(r'corpus_week1.csv', delimiter=',')\n",
    "texts_week2 = pd.read_csv(r'corpus_week2.csv', delimiter=',')\n",
    "texts_week3 = pd.read_csv(r'corpus_week3.csv', delimiter=',')\n",
    "texts_week4 = pd.read_csv(r'corpus_week4.csv', delimiter=',')\n",
    "\n",
    "corpus51 = [list(x) for x in texts_week51.fillna('').values]\n",
    "corpus52 = [list(x) for x in texts_week52.fillna('').values]\n",
    "corpus53 = [list(x) for x in texts_week53.fillna('').values]\n",
    "corpus1 = [list(x) for x in texts_week1.fillna('').values]\n",
    "corpus2 = [list(x) for x in texts_week2.fillna('').values]\n",
    "corpus3 = [list(x) for x in texts_week3.fillna('').values]\n",
    "corpus4 = [list(x) for x in texts_week4.fillna('').values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_week51\n",
    "print(corpus52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build model with test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 01:00:50,469 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2022-04-25 01:00:50,551 : INFO : built Dictionary(5135 unique tokens: ['', 'action', 'already', 'asap', 'country']...) from 4846 documents (total 135688 corpus positions)\n",
      "2022-04-25 01:00:50,552 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary(5135 unique tokens: ['', 'action', 'already', 'asap', 'country']...) from 4846 documents (total 135688 corpus positions)\", 'datetime': '2022-04-25T01:00:50.552640', 'gensim': '4.1.2', 'python': '3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n",
      "2022-04-25 01:00:50,613 : INFO : using symmetric alpha at 0.25\n",
      "2022-04-25 01:00:50,613 : INFO : using symmetric eta at 0.25\n",
      "2022-04-25 01:00:50,614 : INFO : using serial LDA version on this node\n",
      "2022-04-25 01:00:50,619 : INFO : running online (single-pass) LDA training, 4 topics, 1 passes over the supplied corpus of 6019 documents, updating model once every 2000 documents, evaluating perplexity every 6019 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-04-25 01:00:50,620 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2022-04-25 01:00:50,620 : INFO : PROGRESS: pass 0, at document #2000/6019\n",
      "2022-04-25 01:00:51,452 : INFO : merging changes from 2000 documents into a model of 6019 documents\n",
      "2022-04-25 01:00:51,458 : INFO : topic #0 (0.250): 0.803*\"\" + 0.005*\"election\" + 0.003*\"vote\" + 0.002*\"fraud\" + 0.002*\"get\" + 0.002*\"people\" + 0.002*\"fight\" + 0.002*\"stand\" + 0.002*\"trump\" + 0.002*\"need\"\n",
      "2022-04-25 01:00:51,459 : INFO : topic #1 (0.250): 0.044*\"\" + 0.001*\"election\" + 0.000*\"vote\" + 0.000*\"trump\" + 0.000*\"get\" + 0.000*\"state\" + 0.000*\"stand\" + 0.000*\"fight\" + 0.000*\"fraud\" + 0.000*\"thank\"\n",
      "2022-04-25 01:00:51,459 : INFO : topic #2 (0.250): 0.044*\"\" + 0.001*\"election\" + 0.000*\"vote\" + 0.000*\"fraud\" + 0.000*\"stand\" + 0.000*\"trump\" + 0.000*\"fight\" + 0.000*\"people\" + 0.000*\"say\" + 0.000*\"need\"\n",
      "2022-04-25 01:00:51,460 : INFO : topic #3 (0.250): 0.120*\"\" + 0.016*\"voter\" + 0.016*\"fraud\" + 0.014*\"real\" + 0.005*\"stand\" + 0.004*\"election\" + 0.003*\"covid\" + 0.002*\"dem\" + 0.002*\"see\" + 0.002*\"count\"\n",
      "2022-04-25 01:00:51,461 : INFO : topic diff=3.067094, rho=1.000000\n",
      "2022-04-25 01:00:51,461 : INFO : PROGRESS: pass 0, at document #4000/6019\n",
      "2022-04-25 01:00:51,793 : INFO : merging changes from 2000 documents into a model of 6019 documents\n",
      "2022-04-25 01:00:51,795 : INFO : topic #0 (0.250): 0.805*\"\" + 0.005*\"election\" + 0.003*\"vote\" + 0.002*\"get\" + 0.002*\"fraud\" + 0.002*\"people\" + 0.002*\"fight\" + 0.002*\"steal\" + 0.002*\"stand\" + 0.002*\"trump\"\n",
      "2022-04-25 01:00:51,796 : INFO : topic #1 (0.250): 0.018*\"\" + 0.005*\"match\" + 0.003*\"road\" + 0.003*\"accident\" + 0.003*\"eld\" + 0.003*\"tragic\" + 0.002*\"delay\" + 0.001*\"effect\" + 0.001*\"devon\" + 0.001*\"cushion\"\n",
      "2022-04-25 01:00:51,796 : INFO : topic #2 (0.250): 0.017*\"\" + 0.003*\"girl\" + 0.003*\"constitute\" + 0.003*\"rate\" + 0.002*\"amendment\" + 0.002*\"delegation\" + 0.002*\"caoon\" + 0.002*\"progressive\" + 0.002*\"snail\" + 0.002*\"aardvark\"\n",
      "2022-04-25 01:00:51,797 : INFO : topic #3 (0.250): 0.065*\"\" + 0.017*\"real\" + 0.016*\"fraud\" + 0.015*\"voter\" + 0.004*\"election\" + 0.003*\"ballot\" + 0.003*\"window\" + 0.003*\"chief\" + 0.003*\"move\" + 0.003*\"former\"\n",
      "2022-04-25 01:00:51,797 : INFO : topic diff=0.456587, rho=0.707107\n",
      "2022-04-25 01:00:51,798 : INFO : PROGRESS: pass 0, at document #6000/6019\n",
      "2022-04-25 01:00:52,063 : INFO : merging changes from 2000 documents into a model of 6019 documents\n",
      "2022-04-25 01:00:52,066 : INFO : topic #0 (0.250): 0.806*\"\" + 0.005*\"election\" + 0.003*\"vote\" + 0.002*\"fraud\" + 0.002*\"get\" + 0.002*\"people\" + 0.002*\"fight\" + 0.002*\"steal\" + 0.002*\"need\" + 0.002*\"trump\"\n",
      "2022-04-25 01:00:52,067 : INFO : topic #1 (0.250): 0.009*\"\" + 0.008*\"match\" + 0.007*\"conspire\" + 0.006*\"oligarchs_actively\" + 0.005*\"stick\" + 0.005*\"conversation\" + 0.004*\"road\" + 0.004*\"bail\" + 0.004*\"joe\" + 0.003*\"weekend\"\n",
      "2022-04-25 01:00:52,068 : INFO : topic #2 (0.250): 0.009*\"\" + 0.004*\"freak\" + 0.004*\"virus\" + 0.003*\"girl\" + 0.003*\"daily\" + 0.003*\"giant\" + 0.003*\"insult\" + 0.003*\"tidal\" + 0.003*\"accountability\" + 0.003*\"fboloudcom\"\n",
      "2022-04-25 01:00:52,069 : INFO : topic #3 (0.250): 0.043*\"\" + 0.019*\"fraud\" + 0.018*\"voter\" + 0.014*\"real\" + 0.004*\"hang\" + 0.004*\"hea\" + 0.004*\"election\" + 0.004*\"former\" + 0.004*\"ballot\" + 0.004*\"remedy\"\n",
      "2022-04-25 01:00:52,069 : INFO : topic diff=0.317591, rho=0.577350\n",
      "2022-04-25 01:00:52,076 : INFO : -1.765 per-word bound, 3.4 perplexity estimate based on a held-out corpus of 19 documents with 639 words\n",
      "2022-04-25 01:00:52,076 : INFO : PROGRESS: pass 0, at document #6019/6019\n",
      "2022-04-25 01:00:52,083 : INFO : merging changes from 19 documents into a model of 6019 documents\n",
      "2022-04-25 01:00:52,085 : INFO : topic #0 (0.250): 0.820*\"\" + 0.010*\"election\" + 0.003*\"steal\" + 0.003*\"vote\" + 0.003*\"people\" + 0.002*\"stand\" + 0.002*\"president\" + 0.002*\"never\" + 0.002*\"go\" + 0.002*\"let\"\n",
      "2022-04-25 01:00:52,086 : INFO : topic #1 (0.250): 0.006*\"\" + 0.005*\"match\" + 0.004*\"conspire\" + 0.003*\"oligarchs_actively\" + 0.003*\"stick\" + 0.003*\"conversation\" + 0.002*\"road\" + 0.002*\"bail\" + 0.002*\"joe\" + 0.002*\"weekend\"\n",
      "2022-04-25 01:00:52,086 : INFO : topic #2 (0.250): 0.005*\"\" + 0.002*\"freak\" + 0.002*\"virus\" + 0.002*\"girl\" + 0.002*\"daily\" + 0.002*\"giant\" + 0.002*\"insult\" + 0.002*\"tidal\" + 0.002*\"accountability\" + 0.001*\"fboloudcom\"\n",
      "2022-04-25 01:00:52,087 : INFO : topic #3 (0.250): 0.054*\"loyalty\" + 0.053*\"outcome\" + 0.053*\"publish\" + 0.051*\"rioter\" + 0.040*\"manufacturing\" + 0.030*\"\" + 0.021*\"fraud\" + 0.017*\"trade\" + 0.008*\"voter\" + 0.008*\"outline\"\n",
      "2022-04-25 01:00:52,087 : INFO : topic diff=0.250532, rho=0.500000\n",
      "2022-04-25 01:00:52,088 : INFO : LdaModel lifecycle event {'msg': 'trained LdaModel(num_terms=5135, num_topics=4, decay=0.5, chunksize=2000) in 1.47s', 'datetime': '2022-04-25T01:00:52.088743', 'gensim': '4.1.2', 'python': '3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.820*\"\" + 0.010*\"election\" + 0.003*\"steal\" + 0.003*\"vote\" + 0.003*\"people\" + 0.002*\"stand\" + 0.002*\"president\" + 0.002*\"never\" + 0.002*\"go\" + 0.002*\"let\"'),\n",
       " (1,\n",
       "  '0.006*\"\" + 0.005*\"match\" + 0.004*\"conspire\" + 0.003*\"oligarchs_actively\" + 0.003*\"stick\" + 0.003*\"conversation\" + 0.002*\"road\" + 0.002*\"bail\" + 0.002*\"joe\" + 0.002*\"weekend\"'),\n",
       " (2,\n",
       "  '0.005*\"\" + 0.002*\"freak\" + 0.002*\"virus\" + 0.002*\"girl\" + 0.002*\"daily\" + 0.002*\"giant\" + 0.002*\"insult\" + 0.002*\"tidal\" + 0.002*\"accountability\" + 0.001*\"fboloudcom\"'),\n",
       " (3,\n",
       "  '0.054*\"loyalty\" + 0.053*\"outcome\" + 0.053*\"publish\" + 0.051*\"rioter\" + 0.040*\"manufacturing\" + 0.030*\"\" + 0.021*\"fraud\" + 0.017*\"trade\" + 0.008*\"voter\" + 0.008*\"outline\"')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = Dictionary(corpus52)\n",
    "corpus = [dictionary.doc2bow(text) for text in corpus51]\n",
    "\n",
    "np.random.seed(1) # setting random seed to get the same results each time.\n",
    "\n",
    "from gensim.models import ldamodel\n",
    "model = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=4, minimum_probability=1e-8)\n",
    "model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_51 = corpus51[5]\n",
    "doc_52 = corpus52[0]\n",
    "doc_53 = corpus53[0]\n",
    "doc_1 = corpus1[4]\n",
    "doc_2 = corpus2[1]\n",
    "doc_3 = corpus3[0]\n",
    "doc_4 = corpus4[1]\n",
    "\n",
    "\n",
    "# now let's make these into a bag of words format\n",
    "bow_51 = model.id2word.doc2bow(doc_51)   \n",
    "bow_52 = model.id2word.doc2bow(doc_52)   \n",
    "bow_53 = model.id2word.doc2bow(doc_53)\n",
    "bow_1 = model.id2word.doc2bow(doc_1)   \n",
    "bow_2 = model.id2word.doc2bow(doc_2)   \n",
    "bow_3 = model.id2word.doc2bow(doc_3)\n",
    "bow_4 = model.id2word.doc2bow(doc_4)   \n",
    "\n",
    "# we can now get the LDA topic distributions for these\n",
    "lda_bow_51 = model[bow_51]\n",
    "lda_bow_52 = model[bow_52]\n",
    "lda_bow_53 = model[bow_53]\n",
    "lda_bow_1 = model[bow_1]\n",
    "lda_bow_2 = model[bow_2]\n",
    "lda_bow_3 = model[bow_3]\n",
    "lda_bow_4 = model[bow_4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.97848237), (1, 0.0071551995), (2, 0.0071550966), (3, 0.0072073075)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_bow_51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The distance between week 51 and week 52: 0.09184565839857405 \n",
      "\n",
      "The distance between week 52 and week 53: 0.17373862228754475 \n",
      "\n",
      "The distance between week 53 and week 01: 0.10582816926072752 \n",
      "\n",
      "The distance between week 01 and week 02: 0.04817568536396766 \n",
      "\n",
      "The distance between week 02 and week 03: 0.00437014867500873 \n",
      "\n",
      "The distance between week 03 and week 04: 0.005570218415861333 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim.matutils import hellinger\n",
    "\n",
    "print(\"The distance between week 51 and week 52: {} \\n\".format(hellinger(lda_bow_51, lda_bow_52)))\n",
    "print(\"The distance between week 52 and week 53: {} \\n\".format(hellinger(lda_bow_52, lda_bow_53)))\n",
    "print(\"The distance between week 53 and week 01: {} \\n\".format(hellinger(lda_bow_53, lda_bow_1)))\n",
    "print(\"The distance between week 01 and week 02: {} \\n\".format(hellinger(lda_bow_1, lda_bow_2)))\n",
    "print(\"The distance between week 02 and week 03: {} \\n\".format(hellinger(lda_bow_2, lda_bow_3)))\n",
    "print(\"The distance between week 03 and week 04: {} \\n\".format(hellinger(lda_bow_3, lda_bow_4)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cb939e895e011b91cf3716df6a091d9fe82c7b6492242f25d3aa87510cfc200b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
